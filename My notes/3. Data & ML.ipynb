{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Experimental Data\n",
    "* Modelling\n",
    "    * Least squares objective function: $\\sum_{i=0}^{len(observed)-1}(observed[i]-predicted[i])^2$\n",
    "        * ~ variance / # experiments\n",
    "    * Linear regression: find the curve of least square - polyfit\n",
    "        * Assume we want to find a polynomial\n",
    "        * model = pylab.polyfit(xVals, yVals, degree)\n",
    "* Prediction: polyval\n",
    "    * estyVals = pylab.polyval(model, xVals)\n",
    "* Which fit provides more accurate estimate?\n",
    "    * compare two different models for the same data: mean squared error $\\frac{\\sum_{i=0}^{len(observed)-1}(observed[i]-predicted[i])^2)}{len(data)}$\n",
    "    * absolute goodness of fit: coefficient of determination $R^2 = 1 - \\frac{\\sum(y_i-p_i)^2}{\\sum(y_i-\\mu)^2}$, where $y_i$ are measured values, $p_i$ are predicted values, $\\mu$ mean of measured values\n",
    "        * capture the portion of variability in the data is accounted for by my model\n",
    "        * r=1: variability is all accounted for!; r=0: the model does not capture anything\n",
    "    * problem of overfitting: not only fit the underlying process, but also the noise\n",
    "        * cross validation: generate model using one dataset and test them on another dataset\n",
    "            * small data-set: leave-one-out cross validation\n",
    "            * larger data-set\n",
    "                * k-fold cross validation: partition into k equal size sets, model trained on k-1 sets, test on the remaining set\n",
    "                * repeated random sampling: randomly select n elements to train model, test on the remaining elements\n",
    "        * visualise as search process\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, pylab, numpy\n",
    "def plotData(fileName):\n",
    "    xVals, yVals = getData(fileName)\n",
    "    xVals = pylab.array(xVals)\n",
    "    yVals = pylab.array(yVals)\n",
    "    xVals = xVals*9.81  #acc. due to gravity\n",
    "    pylab.plot(xVals, yVals, 'bo',\n",
    "               label = 'Measured displacements')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aveMeanSquareError(data, predicted):\n",
    "    error = 0.0\n",
    "    for i in range(len(data)):\n",
    "        error += (data[i] - predicted[i])**2\n",
    "    return error/len(data)\n",
    "\n",
    "def rSquared(observed, predicted):\n",
    "    error = ((predicted - observed)**2).sum()\n",
    "    meanError = error/len(observed)\n",
    "    return 1 - (meanError/numpy.var(observed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genFits(xVals, yVals, degrees):\n",
    "    models = []\n",
    "    for d in degrees:\n",
    "        model = pylab.polyfit(xVals, yVals, d)\n",
    "        models.append(model)\n",
    "    return models\n",
    "\n",
    "def testFits(models, degrees, xVals, yVals, title):\n",
    "    pylab.plot(xVals, yVals, 'o', label = 'Data')\n",
    "    for i in range(len(models)):\n",
    "        estYVals = pylab.polyval(models[i], xVals)\n",
    "        error = rSquared(yVals, estYVals)\n",
    "        pylab.plot(xVals, estYVals,\n",
    "                   label = 'Fit of degree '\\\n",
    "                   + str(degrees[i])\\\n",
    "                   + ', R2 = ' + str(round(error, 5)))\n",
    "    pylab.legend(loc = 'best')\n",
    "    pylab.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeaveOneOutCrossValidation(dataset):\n",
    "    testResults = []\n",
    "    for i in range(len(dataset)):\n",
    "        training = dataset[:].pop(i)\n",
    "        model = buildModel(training)\n",
    "        testResults.append(test(model, dataset[i]))\n",
    "    avg = sum(testResults)/len(testResults)\n",
    "\n",
    "def RepeatedRandomSampling(dataset, num_trials, num_train):\n",
    "    testResults = []\n",
    "    for i in range(num_trials):\n",
    "        trainX, trainY, testX, testY = splitData(xVals,)\n",
    "        model = buildModel(training)\n",
    "        testResults.append(test(model,testSet))\n",
    "    avg = sum(testResults)/len(testResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning\n",
    "## Introduction\n",
    "* basic paradigm\n",
    "    * observe set of examples: training data\n",
    "    * infer sth about process that generated that data\n",
    "    * use inference to make predictions about previously unseen data: test data\n",
    "    * variationks on paradigm:\n",
    "        * supervised: given a set of feature/label pairs -> classification\n",
    "            * overfitting? trade-off between false positives & false negatives\n",
    "        * unsupervised: given a set of feature vectors (without labels), group them into natural clusters -> clastering\n",
    "* clustering and classification\n",
    "    * clustering: suppose there are k different groups, but unlabeled\n",
    "    * classfication: given labeled groups, WTF the subsurface in that space that separates the groups, subject to constraints on complexity of surface\n",
    "        * \\# of clusters\n",
    "        * complexity of separating surface\n",
    "        * avoid over-fitting\n",
    "* Feature engineering: want to maximise signal to noise ratio (SNR)\n",
    "    * Metric for feature vetors\n",
    "        * Minkowski metric $dist(X1, X2, p) = (\\sum_{k=1}^{len}abs(X1_k - X2_k)^p)^{p-1}$\n",
    "            * usually use Euclidean metric\n",
    "            * Manhattan metric may be appropriate if different dimensions are not comparable\n",
    "        * scale of dimension can be important\n",
    "            * z-scaling\n",
    "            * linear-interpolation\n",
    "* Choose the classfication subsurface - measure the training accuracy of models\n",
    "    * accuracy = (true positive + true negative) / (true positive + true negative + false positive + false negative)\n",
    "    * positive predictive value = true positive / (true positive + false positive)  - percentage correctly identified as positive\n",
    "    * sensitivity = true positive / (true positive + false negative) - percentage correctly found; specificity = true negative / (true negative + false positive) - percentage correctly rejected -> trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-scaling\n",
    "def scaleAttrs(vals):     # scale the feature by Z-scaling s.t. mean = 0, var  = 1; alternative: linear interpolation\n",
    "    vals = pylab.array(vals)\n",
    "    mean = sum(vals)/len(vals)\n",
    "    sd = numpy.std(vals)\n",
    "    vals = vals - mean\n",
    "    return vals/sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "* An optimisation problem: Find a C that minimise dissimilarity, under constraint e.g. minimum distance between clusters/no. of clusters\n",
    "    * $variability(c) = \\sum_{e\\in c}distance(mean(c),e)^2$ \n",
    "        * =variance * no. of clusters: why not using variance? big & bad worse than small & bad\n",
    "    * $dissimilarity(C) = \\sum_{c\\in C}variability(c)$\n",
    "    \n",
    "### hierarchical clustering\n",
    "* Algorithm\n",
    "    1. Start by assigning each item to a cluster \n",
    "    2. Find the closest pair of clusters and merge them into a single cluster\n",
    "    3. Continue until all items are clustered into a single cluster of size N. In this process, a dendogram is created. Can select number of clusters using dendogram. (agglomerative hierarchical clustering)\n",
    "* Linkage metrics\n",
    "    * single-linkage: shortest distance from any member of one cluster to any member of the other cluster\n",
    "    * complete-linkage: greatest distance ...\n",
    "    * average-linkage: average distance ...\n",
    "* How is it?\n",
    "    * Deterministic\n",
    "    * may not be optimal: a greedy algorithm\n",
    "    * flexible wrt linkage criteria\n",
    "    * slow: O(n^3), O(n^2) exists for some linkage criteria\n",
    "    \n",
    "### K-means clustering\n",
    "* most useful when you know how many clusters you want\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomly chose k examples as initial centroids \n",
    "while true: \n",
    "    create k clusters by assigning each example to closest centroid\n",
    "    compute k new centroids by averaging examples in each cluster\n",
    "    if centroids donâ€™t change: \n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* complexity: k * n * d where n is number of pts, d is time required to compute the distance between pts\n",
    "* problems \n",
    "    * choosing the wrong k leads to strange results. can choose k by:\n",
    "        * prior knolwedge about application domain\n",
    "        * search for a good k\n",
    "            * try different k and evaluate quality of results \n",
    "            <img src = \"1.png\" width = \"500\">\n",
    "                * k = 2 - a lot of positive not chosen, bad sensitivity, different characteristics not captured\n",
    "                * k = 6 - total no. of positive chosen not improved\n",
    "            * run hierarchical clustering on subset of data\n",
    "    * results depend on initial centroids -> affect no. of iterations and final answer. can mitigate dependence by:\n",
    "        * make sure they are distrubted over space\n",
    "        * try multiple sets of randomly chosen initial centroids (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try multiple sets of initial centroids\n",
    "best = kMeans(points)\n",
    "for t in range(numTrials):\n",
    "    C = kMeans(points)\n",
    "    if dissmilarity(C) < dissmilarity(best):\n",
    "        best = C\n",
    "return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(examples, k, verbose = False):\n",
    "    #Get k randomly chosen initial centroids, create cluster for each\n",
    "    initialCentroids = random.sample(examples, k)\n",
    "    clusters = []\n",
    "    for e in initialCentroids:\n",
    "        clusters.append(cluster.Cluster([e]))\n",
    "        \n",
    "    #Iterate until centroids do not change\n",
    "    converged = False\n",
    "    numIterations = 0\n",
    "    while not converged:\n",
    "        numIterations += 1\n",
    "        #Create a list containing k distinct empty lists\n",
    "        newClusters = []\n",
    "        for i in range(k):\n",
    "            newClusters.append([])\n",
    "            \n",
    "        #Associate each example with closest centroid\n",
    "        for e in examples:\n",
    "            #Find the centroid closest to e\n",
    "            smallestDistance = e.distance(clusters[0].getCentroid())\n",
    "            index = 0\n",
    "            for i in range(1, k):\n",
    "                distance = e.distance(clusters[i].getCentroid())\n",
    "                if distance < smallestDistance:\n",
    "                    smallestDistance = distance\n",
    "                    index = i\n",
    "            #Add e to the list of examples for appropriate cluster\n",
    "            newClusters[index].append(e)\n",
    "            \n",
    "        for c in newClusters: #Avoid having empty clusters\n",
    "            if len(c) == 0:\n",
    "                raise ValueError('Empty Cluster')\n",
    "        \n",
    "        #Update each cluster; check if a centroid has changed\n",
    "        converged = True\n",
    "        for i in range(k):\n",
    "            if clusters[i].update(newClusters[i]) > 0.0:\n",
    "                converged = False\n",
    "        if verbose:\n",
    "            print('Iteration #' + str(numIterations))\n",
    "            for c in clusters:\n",
    "                print(c)\n",
    "            print('') #add blank line\n",
    "    return clusters\n",
    "\n",
    "def trykmeans(examples, numClusters, numTrials, verbose = False):\n",
    "    \"\"\"Calls kmeans numTrials times and returns the result with the\n",
    "          lowest dissimilarity\"\"\"\n",
    "    best = kmeans(examples, numClusters, verbose)\n",
    "    minDissimilarity = cluster.dissimilarity(best)\n",
    "    trial = 1\n",
    "    while trial < numTrials:\n",
    "        try:\n",
    "            clusters = kmeans(examples, numClusters, verbose)\n",
    "        except ValueError:\n",
    "            continue #If failed, try again\n",
    "        currDissimilarity = cluster.dissimilarity(clusters)\n",
    "        if currDissimilarity < minDissimilarity:\n",
    "            best = clusters\n",
    "            minDissimilarity = currDissimilarity\n",
    "        trial += 1\n",
    "    return best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
